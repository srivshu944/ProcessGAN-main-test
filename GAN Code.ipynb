{"cells":[{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19101,"status":"ok","timestamp":1707836349934,"user":{"displayName":"Anjali Singh","userId":"11448085710217635307"},"user_tz":-60},"id":"qq18Wd0aoXUX","outputId":"666ec255-1160-40c3-db8d-7954e9ddc231"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":658,"status":"ok","timestamp":1707836353276,"user":{"displayName":"Anjali Singh","userId":"11448085710217635307"},"user_tz":-60},"id":"1MRfZ8CRop0n"},"outputs":[],"source":["import os\n","os.chdir('/content/drive/MyDrive/Thesis OM/GAN/GAN/SyntheticDataGAN')"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1707836353949,"user":{"displayName":"Anjali Singh","userId":"11448085710217635307"},"user_tz":-60},"id":"k6Bo7pLxosu3"},"outputs":[],"source":["import math\n","import torch\n","import torch.nn as nn\n","from torch.nn import TransformerEncoder, TransformerEncoderLayer\n","\n","class Generator(nn.Module):\n","    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, padding_index, dropout=0.5):\n","        super(Generator, self).__init__()\n","        self.pos_encoder = PositionalEncoding(ninp, dropout)\n","        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n","        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n","        self.encoder = nn.Embedding(ntoken, ninp, padding_idx=padding_index)\n","        self.ninp = ninp\n","        self.decoder = nn.Linear(ninp, ntoken)\n","        self.init_weights()\n","\n","    def generate_square_subsequent_mask(self, sz):\n","        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n","        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n","        return mask\n","\n","    def init_weights(self):\n","        initrange = 0.1\n","        self.encoder.weight.data.uniform_(-initrange, initrange)\n","        self.decoder.bias.data.zero_()\n","        self.decoder.weight.data.uniform_(-initrange, initrange)\n","\n","    def forward(self, src, src_mask):\n","        src = self.encoder(src)\n","        src = src * math.sqrt(self.ninp)\n","        src = self.pos_encoder(src)\n","        output = self.transformer_encoder(src, src_mask)\n","        output = self.decoder(output)\n","        return output"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1707836355510,"user":{"displayName":"Anjali Singh","userId":"11448085710217635307"},"user_tz":-60},"id":"gKyw7nrxo_kV"},"outputs":[],"source":["import math\n","import torch\n","from torch import nn\n","\n","\n","class PositionalEncoding(nn.Module):\n","    def __init__(self, d_model, dropout=0.1, max_len=5000):\n","        super(PositionalEncoding, self).__init__()\n","        self.dropout = nn.Dropout(p=dropout)\n","        pe = torch.zeros(max_len, d_model)\n","        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n","\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","\n","        if d_model % 2 == 0:\n","            pe[:, 1::2] = torch.cos(position * div_term)\n","        else:\n","            pe[:, 1::2] = torch.cos(position * div_term[:-1])\n","\n","        pe = pe.unsqueeze(0).transpose(0, 1)\n","        self.register_buffer('pe', pe)\n","\n","    def forward(self, x):\n","        x = x + self.pe[:x.size(0), :]\n","        return self.dropout(x)"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1707836357471,"user":{"displayName":"Anjali Singh","userId":"11448085710217635307"},"user_tz":-60},"id":"9E6xvC--o6yL"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from torch.nn import TransformerEncoder, TransformerEncoderLayer\n","\n","class Discriminator(nn.Module):\n","    def __init__(self, dim_out, ninp, nhead, nhid, nlayers, dropout):\n","        super(Discriminator, self).__init__()\n","        self.pos_encoder = PositionalEncoding(ninp, dropout)\n","        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n","        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n","        self.ninp = ninp\n","        self.decoder = nn.Linear(ninp, dim_out)\n","        self.init_weights()\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def generate_square_subsequent_mask(self, sz):\n","        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n","        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n","        return mask\n","\n","    def init_weights(self):\n","        initrange = 0.1\n","        self.decoder.bias.data.zero_()\n","        self.decoder.weight.data.uniform_(-initrange, initrange)\n","\n","    def forward(self, src, src_mask):\n","        src = self.pos_encoder(src)\n","        output = self.transformer_encoder(src, src_mask)\n","        output = output.permute(1,0,2)\n","        output = output[:,-1,:]\n","        output = self.decoder(output)\n","        output = self.sigmoid(output)\n","        return output"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":391,"status":"ok","timestamp":1707836360218,"user":{"displayName":"Anjali Singh","userId":"11448085710217635307"},"user_tz":-60},"id":"F8aLKUANpiak"},"outputs":[],"source":["from torch.utils.data import Dataset\n","import torch\n","\n","\n","class load_ar_data(Dataset):\n","    \"\"\"Dataloader for autoregressive models: LSTM\"\"\"\n","    def __init__(self, Input, Target):\n","        self.Input = Input\n","        self.Target = Target\n","\n","    def __len__(self):\n","        return len(self.Input)\n","\n","    def __getitem__(self, index):\n","        inp = torch.Tensor(self.Input[index])\n","        tar = torch.IntTensor(self.Target[index])\n","        return inp.long(), tar.long()\n","\n","\n","class load_nar_data(Dataset):\n","\n","    def __init__(self, Target):\n","        self.Target = Target\n","\n","    def __len__(self):\n","        return len(self.Target)\n","\n","    def __getitem__(self, index):\n","        tar = torch.IntTensor(self.Target[index])\n","        return tar.long()\n"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1707836361155,"user":{"displayName":"Anjali Singh","userId":"11448085710217635307"},"user_tz":-60},"id":"I3pwrGLzppID"},"outputs":[],"source":["import torch\n","import numpy as np\n","\n","\n","# prepare the dataset for autoregressive models (rnn, transformer-ar)\n","def prepare_ar_data(path, seq_len, vocab, start_token):\n","    f = open(path)\n","    all_seq = []\n","    for line in f:\n","        line = line.split()\n","        seq = []\n","        for i in range(seq_len + 1):\n","            seq.append(int(line[i]) if i < len(line) else (vocab + 1))\n","        all_seq.append(seq)\n","    target = np.array(all_seq)\n","    data_size, seq_len = target.shape\n","    input = np.zeros((data_size, seq_len))\n","    input[:, 0] = start_token\n","    input[:, 1:] = target[:, :seq_len-1]\n","    return input, target\n","\n","\n","# get authentic data for Transformer_Non-autoregressive model\n","def prepare_nar_data(path, seq_len, token_num):\n","    f = open(path)\n","    seq_list = []\n","    for line in f:\n","        line = line.split()\n","        n = len(line)\n","        seq = []\n","        for i in range(seq_len+1):\n","            if i < n:\n","                ind = int(line[i])\n","                seq.append(ind-1)\n","            else:\n","                seq.append(token_num)\n","        seq_list.append(seq)\n","    seqs = np.array(seq_list)\n","    return seqs\n","\n","\n","# prepare the one-hot format authentic data for discriminator of GAN models\n","def prepare_onehot_aut_data(path, ntoken, seq_len):\n","    end_token = ntoken + 1\n","    f = open(path)\n","    onehotdict = []\n","    for line in f:\n","        line = line.split()\n","        seq = []\n","        for i in range(seq_len+1):\n","            onehot = [0 for id in range(end_token)]\n","            if i < len(line):\n","                ind = int(line[i])\n","                onehot[ind-1] = 1\n","                seq.append(onehot)\n","            else:\n","                onehot[end_token-1] = 1\n","                seq.append(onehot)\n","        onehotdict.append(seq)\n","    onehot_data = np.array(onehotdict)\n","    return onehot_data\n","\n","\n","# prepare discriminator labels\n","def prepare_dis_label(size):\n","    pos_label = np.ones(size)\n","    neg_label = np.zeros(size)\n","    pos_label = torch.tensor(pos_label, dtype=torch.float32, requires_grad=False)\n","    neg_label = torch.tensor(neg_label, dtype=torch.float32, requires_grad=False)\n","    return pos_label, neg_label"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":837,"status":"ok","timestamp":1707836363641,"user":{"displayName":"Anjali Singh","userId":"11448085710217635307"},"user_tz":-60},"id":"zLLijUKQ31kE"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","\n","def get_freq_dict2(seqs):\n","    freqdict = {}\n","    for line in seqs:\n","        n = len(line)\n","        if n not in freqdict:\n","            freqdict[n] = 1\n","        else:\n","            freqdict[n] += 1\n","    return freqdict\n","\n","\n","def get_length_stats(seqs):\n","    len_list = []\n","    for seq in seqs:\n","        length = len(seq)\n","        len_list.append(length)\n","\n","    mean = sum(len_list) / len(seqs)\n","    variance = sum([((x - mean) ** 2) for x in len_list]) / len(seqs)\n","    stddev = variance ** 0.5\n","    max_len = max(len_list)\n","    print('max_length',max_len)\n","\n","    return mean, stddev, max_len\n","\n","\n","def write(item, save_path):\n","    with open(save_path + 'dif_log.txt', 'a') as filehandle:\n","        filehandle.write('%s\\n' % item)\n","\n","\n","def save_len_difference(gen_seqs, aut_seqs, save_path):\n","    gen_mean, gen_std, gen_max_len = get_length_stats(gen_seqs)\n","    aut_mean, aut_std, aut_max_len = get_length_stats(aut_seqs)\n","    print(gen_seqs)\n","    print(aut_seqs)\n","    print(save_path)\n","    write('aut_mean: ' + str(aut_mean), save_path)\n","    write('aut_std: ' + str(aut_std), save_path)\n","    write('syn_mean: ' + str(gen_mean), save_path)\n","    write('syn_std: ' + str(gen_std), save_path)\n","\n","    print('syn_mean: ' +str(gen_mean))\n","    print('aut_mean: ' + str(aut_mean))\n","    print('syn_std: ' + str(gen_std))\n","    print('aut_std: ' +str(aut_std))\n","    print('gen max length',gen_max_len)\n","    print('auto max',aut_max_len)\n","\n","    save_len_diff_figure(gen_seqs, aut_seqs, save_path)\n","\n","def save_len_diff_figure(gen_seqs, aut_seqs, save_path):\n","    gen_freq_dict = get_freq_dict2(gen_seqs)\n","    aut_freq_dict = get_freq_dict2(aut_seqs)\n","    print('gen_seqs inside image ',gen_seqs)\n","    print('aut_seqs inside image ',aut_seqs)\n","    print('gen length',gen_freq_dict)\n","    print('auto length',aut_freq_dict)\n","    all_freq_dict = {}\n","    _, _, aut_max_len = get_length_stats(aut_seqs)\n","    for seq_len in aut_freq_dict:\n","        if seq_len not in gen_freq_dict:\n","            all_freq_dict[seq_len] = [aut_freq_dict[seq_len]/len(aut_seqs), 0]\n","        else:\n","            all_freq_dict[seq_len] = [aut_freq_dict[seq_len]/len(aut_seqs), gen_freq_dict[seq_len]/len(gen_seqs)]\n","\n","    for seq_len in gen_freq_dict:\n","\n","        if seq_len not in all_freq_dict:\n","            all_freq_dict[seq_len] = [0, gen_freq_dict[seq_len]/len(gen_seqs)]\n","\n","    my_df = pd.DataFrame([[k, *v] for k, v in all_freq_dict.items()],\n","                         columns=['sequence length', 'authentic processes', 'semi-synthetic processes'])\n","    my_df = my_df.sort_values(by=['sequence length'])\n","    print(my_df)\n","\n","\n","\n","    # draw histogram\n","    SMALL_SIZE = 15\n","    MEDIUM_SIZE = 15\n","    BIGGER_SIZE = 18\n","\n","    plt.rc('font', size=MEDIUM_SIZE)  # controls default text sizes\n","    plt.rc('axes', titlesize=MEDIUM_SIZE)  # fontsize of the axes title\n","    plt.rc('axes', labelsize=MEDIUM_SIZE)  # fontsize of the x and y labels\n","    plt.rc('xtick', labelsize=SMALL_SIZE)  # fontsize of the tick labels\n","    plt.rc('ytick', labelsize=SMALL_SIZE)  # fontsize of the tick labels\n","    plt.rc('legend', fontsize=MEDIUM_SIZE)  # legend fontsize\n","    plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n","\n","    barWidth = 1\n","    bars1 = my_df['authentic processes']\n","    bars2 = my_df['semi-synthetic processes']\n","\n","    max_height = max(bars1.max(), bars2.max()) + 0.02\n","\n","    r1 = np.array(my_df['sequence length'])\n","    r2 = r1\n","\n","    plt.figure(figsize=(10, 6))\n","    plt.bar(r1, bars1, label='Authentic', width=barWidth, color='blue', alpha=0.4)\n","    plt.bar(r2, bars2, label='Synthetic', width=barWidth, color='red', alpha=0.3)\n","\n","    plt.title('Sequence Length Distribution')\n","    plt.xlabel('Sequence Length')\n","    plt.ylabel('Frequency')\n","    plt.xlim(0, 25)\n","    print(aut_max_len)\n","    plt.xticks(np.arange(0, aut_max_len+1, step=1))\n","    plt.xticks(rotation=75)\n","    plt.ylim(0, max_height)\n","    plt.legend()\n","    plt.savefig(save_path + 'length_distribution.png')\n","    plt.show()\n","\n","def get_seqs_from_path(path):\n","    f = open(path)\n","    all_seq = [[int(ind) for ind in line.split()] for line in f]\n","    return all_seq\n","\n","\n","def get_length_dif(aut_path, seq_path, save_path):\n","    aut_seqs = get_seqs_from_path(aut_path)\n","    gen_seqs = get_seqs_from_path(seq_path)\n","    save_len_difference(gen_seqs, aut_seqs, save_path)"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":264,"status":"ok","timestamp":1707836369198,"user":{"displayName":"Anjali Singh","userId":"11448085710217635307"},"user_tz":-60},"id":"jMxz-cHD4LKD"},"outputs":[],"source":["import random\n","import numpy as np\n","import torch\n","import torch.nn.functional as F\n","from matplotlib import pyplot as plt\n","\n","\n","\n","def generate_random_data(bs, vocab_size, seq_len):\n","    rand_data = []\n","    end_token = vocab_size\n","    for i in range(bs):\n","        randomlist = random.choices(range(0, end_token + 1), k=seq_len + 1)\n","        rand_data.append(randomlist)\n","    return rand_data\n","\n","\n","def gen_data_from_rand(size, g_model, ntokens, device, result_file, save_path, seq_len):\n","    gen_list = []\n","    print('gen_data_from_rand')\n","    print('size',size)\n","    print('seq_len',seq_len)\n","    for gen in range(size):\n","        gen_rand_set = generate_random_data(1, ntokens, seq_len)\n","        gen_rand_set = torch.tensor(gen_rand_set, dtype=torch.int64).to(device)\n","        gen_rand_set = torch.transpose(gen_rand_set, 0, 1)\n","        mask_len = gen_rand_set.size()[0]\n","        src_mask = g_model.generate_square_subsequent_mask(mask_len).to(device)\n","        g_output = g_model(gen_rand_set, src_mask)\n","        g_output = g_output.permute(1, 0, 2)\n","        out = F.gumbel_softmax(g_output, tau=1, hard=True)\n","        out_list = out.tolist()\n","        seq = []\n","        for j in range(seq_len + 1):\n","            for k in range(ntokens + 1):\n","                if out_list[0][j][k] == 1:\n","                    seq.append(k)\n","        sub_samp = []\n","        n = len(seq)\n","        for j in range(n):\n","            tok = seq[j]\n","            if tok != ntokens:\n","                sub_samp.append(tok + 1)\n","            else:\n","                break\n","        gen_list.append(sub_samp)\n","    with open(save_path + result_file + '.txt', 'a') as f:\n","        f.writelines(' '.join(str(token) for token in list) + '\\n' for list in gen_list)\n","    return gen_list\n","\n","\n","def write_log(save_path, log, file_name):\n","    with open(save_path + file_name, 'a') as filehandle:\n","        for listitem in log:\n","            filehandle.write('%s\\n' % listitem)\n","\n","\n","def plot_loss(save_path, log, file_name, type):\n","    print(\"loss plot fun\",save_path, log, file_name, type)\n","    fig, ax = plt.subplots()\n","    losses = np.array(log)\n","    if len(losses.shape) == 2:\n","        plt.plot(losses.T[0], label='train loss')\n","        plt.plot(losses.T[1], label='val loss')\n","    else:\n","        plt.plot(losses, label=type)\n","    plt.xlabel('epochs')\n","    plt.ylabel(type)\n","    plt.legend()\n","    fig.savefig(save_path + file_name)\n","\n","def eval_result(save_path, gen_list, test_list):\n","    print(save_path)\n","    save_len_difference(gen_list, test_list, save_path)\n","    print(save_path)\n","    print(\"eval_result func\")\n","    print(gen_list,\"-------------------------------\")\n","    print(test_list,\"-------------------------------\")\n","    save_act_difference(gen_list, test_list, save_path)\n","    save_variance_dif(gen_list, test_list, save_path)\n","\n","def get_pad_mask(output, batch_size, seq_len, vocab_size, padding_ind, device):\n","    out_list = output.tolist()\n","    pad_mask = []\n","    for i in range(batch_size):\n","        pad = seq_len\n","        for j in range(seq_len):\n","            if out_list[i][j][padding_ind] == 1:\n","                pad = j\n","                break\n","        pad_mask.append(pad)\n","    n = len(pad_mask)\n","    pad_mask_mul = []\n","    pad_mask_add = []\n","    for i in range(n):\n","        seq_mul = []\n","        seq_add = []\n","        onehot_one = [1 for _ in range(vocab_size)]\n","        onehot_zero = [0 for _ in range(vocab_size)]\n","        onehot_pad = [0 for _ in range(vocab_size - 1)]\n","        onehot_pad.append(1)\n","        for j in range(seq_len):\n","            if j < pad_mask[i]:\n","                seq_mul.append(onehot_one)\n","                seq_add.append(onehot_zero)\n","            else:\n","                seq_mul.append(onehot_zero)\n","                seq_add.append(onehot_pad)\n","        pad_mask_mul.append(seq_mul)\n","        pad_mask_add.append(seq_add)\n","    pad_mask_mul = torch.tensor(pad_mask_mul, dtype=torch.int64)\n","    pad_mask_add = torch.tensor(pad_mask_add, dtype=torch.int64)\n","    return pad_mask_mul.to(device), pad_mask_add.to(device)\n","\n","\n","def pad_after_end_token(g_output_t, pad_mask_mul, pad_mask_add):\n","    g_output_t = g_output_t * pad_mask_mul\n","    g_output_t = g_output_t + pad_mask_add\n","    g_output_t = g_output_t.permute(1, 0, 2)\n","    return g_output_t\n","\n","\n","def get_act_distribution(g_output, aut_seqs):\n","    g_output_t_act = g_output.sum(0)\n","    g_output_t_act = g_output_t_act.sum(0)\n","    g_authentic_act = aut_seqs.sum(0)\n","    g_authentic_act = g_authentic_act.sum(0)\n","    return g_output_t_act, g_authentic_act\n","\n","\n","def reverse_torch_to_list(seqs, vocab_num):\n","    result = []\n","    for seq in seqs:\n","        seq_i = []\n","        for i in seq:\n","            if i != vocab_num:\n","                seq_i.append(i + 1)\n","            else:\n","                break\n","        result.append(seq_i)\n","    return result\n","\n","\n","def remove_end_token(seqs, vocab_num):\n","    print('inside remove end token')\n","    result = []\n","    for seq in seqs:\n","        seq_i = []\n","        for i in seq:\n","            if i != vocab_num + 1 and i != 0:\n","                seq_i.append(i)\n","            else:\n","                break\n","        result.append(seq_i)\n","        print(result)\n","\n","    return result\n","\n","\n","def write_generated_seqs(save_path, model, gen_seqs):\n","    with open(save_path + 'result_' + model + '.txt', 'a') as f:\n","        f.writelines(' '.join(str(token) for token in list) + '\\n' for list in gen_seqs)"]},{"cell_type":"code","source":[],"metadata":{"id":"ZPTdtaK6-t2x"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":269,"status":"ok","timestamp":1707836371167,"user":{"displayName":"Anjali Singh","userId":"11448085710217635307"},"user_tz":-60},"id":"wE2rMkLZ4gH0"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","\n","def get_freq_dic1(seqs):\n","    freqdict = {}\n","    for line in seqs:\n","        for i in range(len(line)):\n","            ind = line[i]\n","            if ind not in freqdict:\n","                freqdict[ind] = 1\n","            else:\n","                freqdict[ind] += 1\n","    return freqdict\n","\n","\n","def write(item, save_path):\n","    with open(save_path + 'dif_log.txt', 'a') as filehandle:\n","        filehandle.write('%s\\n' % item)\n","\n","\n","def get_act_stats(gen_seqs, aut_seqs):\n","    # build distribution dataframe\n","    gen_lengths =len(gen_seqs)\n","    aut_lengths = len(aut_seqs)\n","    gen_freq_dict = get_freq_dic1(gen_seqs)\n","    aut_freq_dict = get_freq_dic1(aut_seqs)\n","    print(gen_freq_dict)\n","    print(aut_freq_dict)\n","\n","    all_freq_dict = {}\n","\n","    for activity in aut_freq_dict:\n","        if activity not in all_freq_dict:\n","            all_freq_dict[activity] = [aut_freq_dict[activity]]\n","\n","    for activity in all_freq_dict:\n","        if int(activity) in gen_freq_dict:\n","            all_freq_dict[activity].append(gen_freq_dict[int(activity)])\n","        else:\n","            all_freq_dict[activity].append(0)\n","\n","    # build frequency dataframe\n","    frequency_dict = {}\n","    aut_count = 0\n","    gen_count = 0\n","\n","    for act in all_freq_dict:\n","        aut_count += all_freq_dict[act][0]\n","        gen_count += all_freq_dict[act][1]\n","\n","    act_type_distance = 0  # the summation of activity type fraction difference\n","    for act in all_freq_dict:\n","        if act not in frequency_dict:\n","            frequency_dict[act] = [all_freq_dict[act][0] / aut_count, all_freq_dict[act][1] / gen_count]\n","            act_type_distance += abs(all_freq_dict[act][0] / aut_count - all_freq_dict[act][1] / gen_count)\n","    print('act difference: ' + str(act_type_distance))\n","\n","    return act_type_distance, frequency_dict\n","\n","    import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# Mapping from integer codes to string values\n","activity_mapping = {\n","    1: 'A_ACCEPTED',\n","    2: 'A_ACTIVATED',\n","    3: 'A_APPROVED',\n","    4: 'A_CANCELLED',\n","    5: 'A_DECLINED',\n","    6: 'A_FINALIZED',\n","    7: 'A_PARTLYSUBMITTED',\n","    8: 'A_PREACCEPTED',\n","    9: 'A_REGISTERED',\n","    10: 'A_SUBMITTED',\n","    11: 'O_ACCEPTED',\n","    12: 'O_CANCELLED',\n","    13: 'O_CREATED',\n","    14: 'O_DECLINED',\n","    15: 'O_SELECTED',\n","    16: 'O_SENT',\n","    17: 'O_SENT_BACK',\n","    18: 'W_Afhandelen leads',\n","    19: 'W_Beoordelen fraude',\n","    20: 'W_Completeren aanvraag',\n","    21: 'W_Nabellen incomplete dossiers',\n","    22: 'W_Nabellen offertes',\n","    23: 'W_Valideren aanvraag',\n","    24: 'W_Wijzigen contractgegevens'\n","}\n","\n","def save_act_dif_figure(frequency_dict, save_path):\n","    # Convert frequency_dict's keys to string values based on mapping\n","    updated_freq_dict = {activity_mapping[int(k)]: v for k, v in frequency_dict.items()}\n","\n","    my_df = pd.DataFrame([[k, *v] for k, v in updated_freq_dict.items()], columns=['activity', 'real', 'syn'])\n","\n","    # Sorting by 'real' frequency for plotting\n","    df_sorted = my_df.sort_values(by='real', ascending=False)\n","    real_color = '#76b5c5'  # A softer blue\n","    syn_color = '#f2a65a'   # A softer orange\n","    # Set the figure size for better visibility\n","    plt.figure(figsize=(30, 15))\n","\n","    # Adjusting the bar width and positions\n","    bar_width = 0.35\n","    index_sorted = np.arange(len(df_sorted['activity']))\n","\n","    # Adjusting the plot with different colors and a slightly different look\n","\n","    # Set the figure size for better visibility\n","\n","    # New colors for the bars\n","    real_color = '#76b5c5'  # A softer blue\n","    syn_color = '#f2a65a'   # A softer orange\n","\n","    # Create horizontal bars with new colors and adjusted layout for a fresh look\n","    plt.barh(index_sorted, df_sorted['real'], bar_width, label='Real', color=real_color, edgecolor='grey')\n","    plt.barh(index_sorted + bar_width, df_sorted['syn'], bar_width, label='Synthetic', color=syn_color, edgecolor='grey')\n","\n","    # Adding some design tweaks for a more modern look\n","    plt.yticks(index_sorted + bar_width / 2, df_sorted['activity'], fontsize=10)\n","    plt.xlabel('Frequency', fontsize=12)\n","    plt.title('Comparison of Activity Frequency between Real and Synthetic Data', fontsize=14)\n","    plt.legend()\n","\n","    # Adding grid for better readability\n","    plt.grid(color='grey', linestyle='--', linewidth=0.5, axis='x')\n","    plt.gca().invert_yaxis()\n","    plt.savefig(save_path + 'act_distribution.png')\n","\n","    # Adjust layout\n","    plt.tight_layout()\n","\n","    plt.show()\n","\n","\n","# Example usage (replace 'frequency_dict' and 'save_path' with your actual data and desired save location)\n","# save_act_dif_figure(frequency_dict, '/path/to/save/')\n","\n","\n","# def save_act_dif_figure(frequency_dict, save_path):\n","#     print(frequency_dict)\n","#     my_df = pd.DataFrame([[k, *v] for k, v in frequency_dict.items()], columns=['activity', 'real', 'syn'])\n","#     print(my_df,\"_____________________\")\n","#     # draw histogram\n","#     df_sorted = my_df.sort_values(by='real', ascending=False)\n","\n","#     # Set the figure size for better visibility\n","#     plt.figure(figsize=(10, 6))\n","#     bar_width = 0.35\n","#     # Set up the bar widths and positions after sorting\n","#     index_sorted = np.arange(len(df_sorted['activity']))\n","\n","#     # Create the bars for the sorted plot\n","#     plt.bar(index_sorted, df_sorted['real'], bar_width, label='Real', color='b')\n","#     plt.bar(index_sorted + bar_width, df_sorted['syn'], bar_width, label='Synthetic', color='orange')\n","\n","#     # Set the plot details for the sorted data\n","#     plt.xlabel('Activity Type')\n","#     plt.ylabel('Frequency')\n","#     plt.title('Comparison of Activity Frequency between Real and Synthetic Data (Sorted by Real)')\n","#     plt.xticks(index_sorted + bar_width / 2, df_sorted['activity'])\n","#     plt.legend()\n","#     plt.savefig(save_path + 'act_distribution.png')\n","\n","\n","#     # Show the sorted plot\n","#     plt.show()\n","\n","def save_act_difference(gen_seqs, aut_seqs, save_path):\n","    act_type_distance, frequency_dict = get_act_stats(gen_seqs, aut_seqs)\n","    write('act difference: ' + str(act_type_distance), save_path)\n","    print('save_act_dif_figure',)\n","    save_act_dif_figure(frequency_dict, save_path)\n","\n","\n","def get_seqs_from_path(path):\n","    print('path inside eval',path)\n","    f = open(path)\n","    all_seq = [[int(ind) for ind in line.split()] for line in f]\n","    return all_seq\n","\n","\n","def get_act_dif(aut_path, gen_path, save_path):\n","    print('aut_seqs',aut_seqs)\n","    print('gen_seqs',gen_seqs)\n","    aut_seqs = get_seqs_from_path(aut_path)\n","    gen_seqs = get_seqs_from_path(gen_path)\n","    save_act_difference(gen_seqs, aut_seqs, save_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dqoY97XgNdu0"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":240,"status":"ok","timestamp":1707836376734,"user":{"displayName":"Anjali Singh","userId":"11448085710217635307"},"user_tz":-60},"id":"unHty0LcK2Wz"},"outputs":[],"source":["import editdistance\n","\n","\n","def write(item, save_path):\n","    with open(save_path + 'dif_log.txt', 'a') as filehandle:\n","        filehandle.write('%s\\n' % item)\n","\n","\n","def get_variance(seqs):\n","    num_seq = len(seqs)\n","    ed_m = [[0 for _ in range(0, num_seq)] for _ in range(0, num_seq)]\n","    ed_m_norm = [[0 for _ in range(0, num_seq)] for _ in range(0, num_seq)]\n","\n","    variance = 0\n","\n","    for i in range(0, num_seq):\n","        for j in range(0, num_seq):\n","            ed_m[i][j] = editdistance.eval(seqs[i], seqs[j])\n","            if len(seqs[i]) == 0 and len(seqs[j]) == 0:\n","                ed_m_norm[i][j] = 0\n","            else:\n","                ed_m_norm[i][j] = ed_m[i][j] / (len(seqs[i]) + len(seqs[j]))\n","            variance += ed_m_norm[i][j]\n","    variance = variance / (2 * num_seq * num_seq)\n","\n","    return variance\n","\n","\n","def save_variance_dif(gen_seqs, aut_seqs, save_path):\n","    gen_variance = get_variance(gen_seqs)\n","    aut_variance = get_variance(aut_seqs)\n","    diff = aut_variance - gen_variance\n","    write('aut_variance: ' + str(aut_variance), save_path)\n","    write('syn_variance: ' + str(gen_variance), save_path)\n","    write('variance difference: ' + str(diff), save_path)\n","    write('\\n', save_path)\n","    print('aut_variance: ' + str(aut_variance))\n","    print('syn_variance: ' + str(gen_variance))\n","\n","\n","def get_seq_from_path(path):\n","    f = open(path)\n","    all_seq = [[int(ind) for ind in line.split()] for line in f]\n","    return all_seq\n","\n","\n","def get_variance_dif(aut_path, gen_path, save_path):\n","    aut_seqs = get_seq_from_path(aut_path)\n","    gen_seqs = get_seq_from_path(gen_path)\n","    save_variance_dif(gen_seqs, aut_seqs, save_path)"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":274,"status":"ok","timestamp":1707836381813,"user":{"displayName":"Anjali Singh","userId":"11448085710217635307"},"user_tz":-60},"id":"MMPNh-9dpvy0"},"outputs":[],"source":["import random\n","import numpy as np\n","import torch\n","import torch.nn.functional as F\n","from matplotlib import pyplot as plt\n","\n","\n","\n","def generate_random_data(bs, vocab_size, seq_len):\n","    rand_data = []\n","    end_token = vocab_size\n","    for i in range(bs):\n","        randomlist = random.choices(range(0, end_token + 1), k=seq_len + 1)\n","        rand_data.append(randomlist)\n","    return rand_data\n","\n","\n","def gen_data_from_rand(size, g_model, ntokens, device, result_file, save_path, seq_len):\n","    gen_list = []\n","    print('gen_data_from_rand')\n","    print('size',size)\n","    print('seq_len',seq_len)\n","    for gen in range(size):\n","        gen_rand_set = generate_random_data(1, ntokens, seq_len)\n","        gen_rand_set = torch.tensor(gen_rand_set, dtype=torch.int64).to(device)\n","        gen_rand_set = torch.transpose(gen_rand_set, 0, 1)\n","        mask_len = gen_rand_set.size()[0]\n","        src_mask = g_model.generate_square_subsequent_mask(mask_len).to(device)\n","        g_output = g_model(gen_rand_set, src_mask)\n","        g_output = g_output.permute(1, 0, 2)\n","        out = F.gumbel_softmax(g_output, tau=1, hard=True)\n","        out_list = out.tolist()\n","        seq = []\n","        for j in range(seq_len + 1):\n","            for k in range(ntokens + 1):\n","                if out_list[0][j][k] == 1:\n","                    seq.append(k)\n","        sub_samp = []\n","        n = len(seq)\n","        for j in range(n):\n","            tok = seq[j]\n","            if tok != ntokens:\n","                sub_samp.append(tok + 1)\n","            else:\n","                break\n","        gen_list.append(sub_samp)\n","    with open(save_path + result_file + '.txt', 'a') as f:\n","        f.writelines(' '.join(str(token) for token in list) + '\\n' for list in gen_list)\n","    return gen_list\n","\n","\n","def write_log(save_path, log, file_name):\n","    with open(save_path + file_name, 'a') as filehandle:\n","        for listitem in log:\n","            filehandle.write('%s\\n' % listitem)\n","\n","\n","def plot_loss(save_path, log, file_name, type):\n","    print(\"loss plot fun\",save_path, log, file_name, type)\n","    fig, ax = plt.subplots()\n","    losses = np.array(log)\n","    if len(losses.shape) == 2:\n","        plt.plot(losses.T[0], label='train loss')\n","        plt.plot(losses.T[1], label='val loss')\n","    else:\n","        plt.plot(losses, label=type)\n","    plt.xlabel('epochs')\n","    plt.ylabel(type)\n","    plt.legend()\n","    fig.savefig(save_path + file_name)\n","\n","def eval_result(save_path, gen_list, test_list):\n","    print(save_path)\n","    save_len_difference(gen_list, test_list, save_path)\n","    print(save_path)\n","    print(\"eval_result func\")\n","    print(gen_list,\"-------------------------------\")\n","    print(test_list,\"-------------------------------\")\n","    save_act_difference(gen_list, test_list, save_path)\n","    save_variance_dif(gen_list, test_list, save_path)\n","\n","def get_pad_mask(output, batch_size, seq_len, vocab_size, padding_ind, device):\n","    out_list = output.tolist()\n","    pad_mask = []\n","    for i in range(batch_size):\n","        pad = seq_len\n","        for j in range(seq_len):\n","            if out_list[i][j][padding_ind] == 1:\n","                pad = j\n","                break\n","        pad_mask.append(pad)\n","    n = len(pad_mask)\n","    pad_mask_mul = []\n","    pad_mask_add = []\n","    for i in range(n):\n","        seq_mul = []\n","        seq_add = []\n","        onehot_one = [1 for _ in range(vocab_size)]\n","        onehot_zero = [0 for _ in range(vocab_size)]\n","        onehot_pad = [0 for _ in range(vocab_size - 1)]\n","        onehot_pad.append(1)\n","        for j in range(seq_len):\n","            if j < pad_mask[i]:\n","                seq_mul.append(onehot_one)\n","                seq_add.append(onehot_zero)\n","            else:\n","                seq_mul.append(onehot_zero)\n","                seq_add.append(onehot_pad)\n","        pad_mask_mul.append(seq_mul)\n","        pad_mask_add.append(seq_add)\n","    pad_mask_mul = torch.tensor(pad_mask_mul, dtype=torch.int64)\n","    pad_mask_add = torch.tensor(pad_mask_add, dtype=torch.int64)\n","    return pad_mask_mul.to(device), pad_mask_add.to(device)\n","\n","\n","def pad_after_end_token(g_output_t, pad_mask_mul, pad_mask_add):\n","    g_output_t = g_output_t * pad_mask_mul\n","    g_output_t = g_output_t + pad_mask_add\n","    g_output_t = g_output_t.permute(1, 0, 2)\n","    return g_output_t\n","\n","\n","def get_act_distribution(g_output, aut_seqs):\n","    g_output_t_act = g_output.sum(0)\n","    g_output_t_act = g_output_t_act.sum(0)\n","    g_authentic_act = aut_seqs.sum(0)\n","    g_authentic_act = g_authentic_act.sum(0)\n","    return g_output_t_act, g_authentic_act\n","\n","\n","def reverse_torch_to_list(seqs, vocab_num):\n","    result = []\n","    for seq in seqs:\n","        seq_i = []\n","        for i in seq:\n","            if i != vocab_num:\n","                seq_i.append(i + 1)\n","            else:\n","                break\n","        result.append(seq_i)\n","    return result\n","\n","\n","def remove_end_token(seqs, vocab_num):\n","    print('inside remove end token')\n","    result = []\n","    for seq in seqs:\n","        seq_i = []\n","        for i in seq:\n","            if i != vocab_num + 1 and i != 0:\n","                seq_i.append(i)\n","            else:\n","                break\n","        result.append(seq_i)\n","        print(result)\n","\n","    return result\n","\n","\n","def write_generated_seqs(save_path, model, gen_seqs):\n","    with open(save_path + 'result_' + model + '.txt', 'a') as f:\n","        f.writelines(' '.join(str(token) for token in list) + '\\n' for list in gen_seqs)"]},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":283,"status":"ok","timestamp":1707849347979,"user":{"displayName":"Anjali Singh","userId":"11448085710217635307"},"user_tz":-60},"id":"4FcraGcXpWnm"},"outputs":[],"source":["import datetime\n","import random\n","import torch\n","import time\n","import numpy as np\n","import torch.nn.functional as F\n","from torch import nn\n","from torch.utils.data import Dataset, DataLoader\n","from datetime import datetime, timedelta\n","import random\n","\n","\n","class GANs:\n","    def __init__(self, res_path, save_path, model, config, gen_num):\n","        \"\"\"GAN Model and the variants\n","\n","        Parameters:\n","            'seq_len'   : the longest sequence length in data\n","            'vocab_num' : the size of vocabulary\n","            'emb_size'  : embedding dimension\n","            'n_hid'     : the dimension of the feedforward network model in nn.TransformerEncoder\n","            'n_layer'   : the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n","            'n_head_g'  : the number of heads in the multi-head-attention models of generator\n","            'n_head_d'  : the number of heads in the multi-head-attention models of discriminator\n","            'drop_out'  : the dropout value\n","            'gd_ratio'  : k value: the generator updates k times and discriminator updates 1 time\n","            'lr_gen'    : generator learning rate\n","            'lr_dis'    : discriminator learning rate\n","            'epochs'    : total epochs\n","\n","        \"\"\"\n","        self.res_path = res_path\n","        self.save_path = save_path\n","        self.gen_num = gen_num\n","        self.mode = model\n","\n","\n","        # print(res_path,save_path,mode,config['seq_len'])\n","        self.seq_len = 25\n","        self.vocab_num = config['vocab_num']\n","        self.emb_size = config['emb_size']\n","        self.n_hid = config['n_hid']\n","        self.n_layer = config['n_layer']\n","        self.n_head_g = config['n_head_g']\n","        self.n_head_d = config['n_head_d']\n","        self.drop_out = config['drop_out']\n","        self.gd_ratio = config['gd_ratio']\n","        self.lr_gen = config['lr_gen']\n","        self.lr_dis = config['lr_dis']\n","        self.batch_size = config['batch_size']\n","        self.epochs = config['epochs']\n","        self.seed = config['seed']\n","        self.device = config['device']\n","        self.test_size = config['test_size']\n","        self.valid_size = config['valid_size']\n","        self.train_size = config['train_size']\n","        self.n_inp = self.vocab_num + 1\n","        self.pad_ind = self.vocab_num\n","        self.gen_losses = []\n","        self.dis_losses = []\n","        self.d_accuracies = []\n","        self.gen_accuracies=[]\n","\n","    def get_training_metrics(self):\n","        return self.gen_losses, self.dis_losses, self.d_accuracies, self.gen_accuracies\n","\n","    def train(self, target, aut_data):\n","        random.seed(self.seed)\n","        np.random.seed(self.seed)\n","\n","        # initialize generator\n","        g_model = Generator(self.n_inp, self.emb_size, self.n_head_g, self.n_hid, self.n_layer, self.pad_ind, self.drop_out).to(self.device)\n","        # the optimizer of generator\n","\n","        print('.....',self.emb_size,self.n_head_g)\n","        gd_optimizer = torch.optim.Adam(g_model.parameters(), lr=self.lr_gen, betas=(0.5, 0.999))\n","\n","        # initialize discriminator\n","        emsize_d = self.vocab_num + 1\n","        dis_output_dim = 1\n","        d_model = Discriminator(dis_output_dim, emsize_d, self.n_head_d, self.n_hid, self.n_layer, self.drop_out).to(self.device)\n","        d_criterion = nn.BCELoss()\n","        d_optimizer = torch.optim.Adam(d_model.parameters(), lr=self.lr_dis, betas=(0.5, 0.999))\n","\n","        # record the parameters\n","        para_list = [self.vocab_num, self.emb_size, self.n_head_g, self.n_hid, self.n_layer, self.pad_ind, self.drop_out, self.n_head_d,  self.batch_size, self.epochs, self.lr_dis]\n","        write_log(self.save_path, para_list, 'parameter_log.txt')\n","\n","        # load the one-hot format of training data\n","        dataset = load_nar_data(target)\n","        train_data, _, _ = torch.utils.data.random_split(dataset, (self.train_size, self.valid_size, self.test_size), generator=torch.Generator().manual_seed(self.seed))\n","        train_dataloader = DataLoader(train_data, batch_size=self.batch_size, drop_last=False, shuffle=False, num_workers=1)\n","\n","        # load the original format of test data for activity loss calculation\n","        dataset_2 = load_nar_data(aut_data)\n","        _, _, test_data = torch.utils.data.random_split(dataset_2, (self.train_size, self.valid_size, self.test_size), generator=torch.Generator().manual_seed(self.seed))\n","        test_dataloader = DataLoader(test_data, batch_size=self.batch_size, drop_last=False, shuffle=False, num_workers=1)\n","        test_seqs = next(iter(test_dataloader)).tolist()\n","        test_list = reverse_torch_to_list(test_seqs, self.vocab_num)\n","\n","        # run pre epochs if add activity loss\n","        pre_epoch = 25\n","\n","        # log the discriminator's accuracies\n","        d_acc = []\n","\n","        mean_act_loss, mean_gen_loss = self.get_pre_exp_loss(pre_epoch, g_model, d_model, train_dataloader)\n","\n","        for big_epoch in range(1,  self.epochs + 1):\n","            start_time = time.time()\n","            g_model.train()\n","            d_model.train()\n","\n","            dis_total_loss = 0\n","            gen_total_loss = 0\n","\n","            # generate random sequences for generator input\n","            rand_set = generate_random_data(self.train_size, self.vocab_num, self.seq_len)\n","            rand_set = torch.tensor(rand_set, dtype=torch.int64).to(self.device)\n","\n","            acc_i = 0\n","\n","            for i, item in enumerate(train_dataloader):\n","                # update generator\n","                dis_data_pos = item\n","                dis_data_pos = dis_data_pos.to(self.device)\n","                batch = dis_data_pos.size()[0]\n","\n","                # [LENGTH, BATCH_SIZE, VOCAB]\n","                dis_data_pos = dis_data_pos.permute(1, 0, 2)\n","                real_labels = torch.ones(batch, 1).to(self.device)\n","                random_data = rand_set[i:i + batch]\n","                random_data = torch.transpose(random_data, 0, 1)\n","\n","                # generate sequences from random_data\n","                gd_optimizer.zero_grad()\n","                gen_loss, g_output_t = self.generator(random_data, g_model, d_model, batch, d_criterion, real_labels)\n","                g_output_t_act, g_authentic_act = get_act_distribution(g_output_t, dis_data_pos)\n","                act_loss = self.get_act_loss(g_output_t_act, g_authentic_act, batch)\n","                # Inside the training loop, after computing gen_loss\n","\n","                gen_loss = gen_loss / (mean_gen_loss)\n","                # back-propagate the generator\n","                g_loss = act_loss + gen_loss\n","                g_loss.backward()\n","                gd_optimizer.step()\n","                gen_total_loss += g_loss\n","\n","                # update discriminator\n","                if big_epoch % self.gd_ratio == 0:\n","                    d_optimizer.zero_grad()\n","                    dis_loss, gd_acc_neg, gd_acc_pos = self.discrminator(dis_data_pos, g_output_t, g_model, d_model, d_criterion, batch)\n","                    dis_total_loss += dis_loss\n","\n","                    # back-propagate the discriminator\n","                    dis_loss.backward()\n","                    d_optimizer.step()\n","                    acc_i += (gd_acc_neg + gd_acc_pos)/2\n","\n","\n","            if big_epoch % self.gd_ratio == 0:\n","                end_time = time.time()\n","                variance = 0.\n","                acc = acc_i/len(train_dataloader)\n","                #  * random.uniform(0.5, 0.8)\n","                print(\":::::::::::::::::::::accuracy disc\",acc)\n","\n","                d_acc.append(acc.detach().cpu())\n","\n","                gen_total_loss = gen_total_loss/len(train_dataloader)\n","                dis_total_loss = dis_total_loss/len(train_dataloader)\n","\n","\n","                    # Store the averages for plotting\n","                self.gen_losses.append(gen_total_loss.item())  # Assuming gen_total_loss is a tensor\n","                self.dis_losses.append(dis_total_loss.item())\n","                self.d_accuracies.append(acc.item())\n","                print('ad epoch {:3d} |  g_loss {:5.4f} | d_loss {:5.4f} | d_acc_real {:5.2f} | d_acc_fake {:5.2f} | d_acc {:5.2f}'\n","                        .format(big_epoch, gen_total_loss, dis_total_loss, gd_acc_pos, gd_acc_neg, acc))\n","\n","            # generate and evaluate samples every 50 epochs\n","            if big_epoch % 50 == 0:\n","                torch.save(g_model, self.save_path + str(big_epoch)+'g_model.pt')\n","                torch.save(d_model, self.save_path + str(big_epoch)+'d_model.pt')\n","                plot_loss(self.save_path, d_acc, str(big_epoch)+'d_acc.png', 'd_acc')\n","                g_model.eval()\n","\n","                # generate synthetic sequences using the generator and save the sequences\n","                with torch.no_grad():\n","                    gen_list = gen_data_from_rand(self.gen_num, g_model, self.vocab_num, self.device, str(big_epoch) + '_result_trans', self.save_path, self.seq_len)\n","                # evaluate and record the results\n","                with open(self.save_path +'stats/' + 'dif_log.txt', 'a') as filehandle:\n","                    filehandle.write('%s\\n' % big_epoch)\n","                eval_result(self.save_path +'stats/', gen_list, test_list)\n","\n","    def get_act_loss(self, g_output_t_act, g_authentic_act, batch_size):\n","        \"\"\"get the additional activity distribution loss between generated sequences and real sequences\"\"\"\n","        if self.mode == \"gan\":\n","            act_loss_criterion = nn.MSELoss()\n","            act_loss = act_loss_criterion(g_output_t_act.float(), g_authentic_act.float()) / (batch_size)\n","        else:\n","            act_loss = 0\n","\n","        # return the activity distribution loss\n","        return act_loss\n","    import matplotlib as plt\n","\n","\n","    def generator(self, data, g_model, d_model, batch, d_criterion, real_labels):\n","        \"\"\"Transformer encoder-based Generator\"\"\"\n","        mask_len = data.size()[0]\n","        src_mask = g_model.generate_square_subsequent_mask(mask_len).to(self.device)\n","        g_output = g_model(data, src_mask)\n","        g_output_st = g_output.permute(1, 0, 2)\n","\n","        # use straight-through Gumbel-softmax to obtain gradient from discriminator\n","        g_output_t = F.gumbel_softmax(g_output_st, tau=1, hard=True)\n","\n","        # the tokens generated after the end token will be padded\n","        pad_mask_mul, pad_mask_add = get_pad_mask(g_output_t, batch, self.seq_len + 1, self.vocab_num + 1, self.pad_ind, self.device)\n","        g_output_t = pad_after_end_token(g_output_t, pad_mask_mul, pad_mask_add)\n","\n","        # generator loss is given by discriminator's prediction\n","        d_predict = d_model(g_output_t, src_mask)\n","        gen_loss = d_criterion(d_predict, real_labels)\n","        gen_accuracy = (d_predict > 0.5).float().mean().item()\n","        self.gen_accuracies.append(gen_accuracy)\n","\n","        # return the generator loss, and the generated sequences\n","        return gen_loss, g_output_t\n","\n","    def discrminator(self, dis_data_pos, g_output_t, g_model, d_model, d_criterion, batch):\n","        \"\"\"Transformer encoder-based Discriminator\"\"\"\n","        mask_len = dis_data_pos.size()[0]\n","        src_mask = g_model.generate_square_subsequent_mask(mask_len).to(self.device)\n","        dis_label_pos, dis_label_neg = prepare_dis_label(batch)\n","        dis_label_neg = dis_label_neg.to(self.device)\n","        dis_label_pos = dis_label_pos.to(self.device)\n","\n","        dis_predict_pos = d_model(dis_data_pos, src_mask)\n","        dis_predict_neg = d_model(g_output_t.detach(), src_mask)\n","        dis_loss_pos = d_criterion(dis_predict_pos, dis_label_pos.reshape(-1, 1))\n","        dis_loss_neg = d_criterion(dis_predict_neg, dis_label_neg.reshape(-1, 1))\n","\n","        predict_neg = (dis_predict_neg.flatten().round())\n","        gd_acc_neg = (predict_neg == dis_label_neg.flatten()).sum() / batch\n","        predict_pos = (dis_predict_pos.flatten().round())\n","        gd_acc_pos = (predict_pos == dis_label_pos.flatten()).sum() / batch\n","\n","        dis_loss = dis_loss_pos + dis_loss_neg\n","\n","        # return the discriminator loss, the accuracy of negative samples and positive samples\n","        return dis_loss, gd_acc_neg, gd_acc_pos\n","\n","    def get_pre_exp_loss(self, pre_epoch, g_model, d_model, train_dataloader):\n","        \"\"\"Calculate the expectation loss values.\n","        Generator loss and activity distribution loss are expected in the same scale when added together.\n","        total_loss = weight*act_loss + gen_loss\n","        where:\n","        weight -> mean(gen_loss)/mean(act_loss)\n","        \"\"\"\n","        total_act_loss = 0\n","        total_gen_loss = 0\n","        d_criterion = nn.BCELoss()\n","        for epoch in range(pre_epoch):\n","            print('pre epoch' + str(epoch))\n","            g_model.train()\n","            d_model.train()\n","            rand_set = generate_random_data(self.train_size, self.vocab_num, self.seq_len)\n","            rand_set = torch.tensor(rand_set, dtype=torch.int64).to(self.device)\n","            for i, item in enumerate(train_dataloader):\n","                dis_data_pos = item\n","                dis_data_pos = dis_data_pos.to(self.device)\n","                batch = dis_data_pos.size()[0]\n","                real_labels = torch.ones(batch, 1).to(self.device)\n","                data = rand_set[i:i + batch]\n","                data = torch.transpose(data, 0, 1)\n","                gen_loss, pre_g_output_t = self.generator(data, g_model, d_model, batch, d_criterion, real_labels)\n","                pre_g_output_t_act, g_authentic_act = get_act_distribution(pre_g_output_t, dis_data_pos)\n","                act_loss = self.get_act_loss(pre_g_output_t_act, g_authentic_act, batch)\n","                total_act_loss += act_loss.item()\n","                total_gen_loss += gen_loss.item()\n","        mean_act = total_act_loss / pre_epoch\n","        mean_gen = total_gen_loss / pre_epoch\n","\n","\n","\n","        # return the mean value of the activity distribution loss, and the generator loss\n","        return mean_act, mean_gen\n","\n","    def run(self):\n","        aut_onehot_data = prepare_onehot_aut_data(self.res_path, self.vocab_num, self.seq_len)\n","        aut_data = prepare_nar_data(self.res_path, self.seq_len, self.vocab_num)\n","        self.train(aut_onehot_data, aut_data)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1dOJyhiwAdPjULd4qtxZz3_JwBf7TVRXI"},"id":"sC2_hyOWp10E","outputId":"109e81d8-0507-40cb-c138-9f3d22d1a77f"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["import os\n","import torch\n","import numpy as np\n","import random\n","from datetime import datetime\n","import csv\n","\n","# Set the random seed for reproducibility\n","seed = 88\n","np.random.seed(seed)\n","random.seed(seed)\n","torch.manual_seed(seed)\n","\n","\n","def get_config():\n","    \"\"\"\n","    Returns the configuration for the LSTM model.\n","    \"\"\"\n","    return {\n","\n","        'train_size': 6917,\n","        'test_size' : 865,\n","        'valid_size': 865,\n","        'seq_len'   : 25,    # the longest sequence length in data\n","        'vocab_num' : 24,\n","        'emb_size'  : 8,   # embedding dimension\n","        'n_hid'     : 32,  # the dimension of the feedforward network model in nn.TransformerEncoder\n","        'n_layer'   : 2,   # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n","        'n_head_g'  : 4,   # the number of heads in the multi-head-attention models of generator\n","        'n_head_d'  : 1,   # the number of heads in the multi-head-attention models of discriminator\n","        'drop_out'  : 0.1, # the dropout value\n","        'batch_size': 32,\n","        'gd_ratio'  : 2,       # k value: the generator updates k times and discriminator updates 1 time\n","        'lr_gen'    : 0.001,  # generator learning rate\n","        'lr_dis'    : 0.001,  # discriminator learning rate\n","        'epochs'    : 600,    # total epochs\n","        'seed'      : seed,\n","        'device'    : torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n","\n","    }\n","def run():\n","    # Define your dataset and model details directly\n","    seq_path = 'Dataset/bpi25.txt'\n","    model_name = 'gan'\n","    gen_num = 1000\n","\n","    # Generate a timestamped directory to save the results\n","    dateTimeObj = datetime.now()\n","    save_time = dateTimeObj.strftime(\"%m-%d-%H-%M\")\n","    save_path = f'result/{save_time}_{model_name}/'\n","\n","    os.makedirs(save_path, exist_ok=True)\n","    save_path_res = save_path + 'stats/'\n","    print(save_path_res)\n","    os.makedirs(os.path.dirname(save_path_res), exist_ok=True)\n","\n","    config = get_config()\n","\n","    print(config['seq_len'])\n","    gan = GANs(seq_path, save_path, model_name, config, gen_num)\n","    print(gan)\n","\n","    gan.run()\n","    gen_losses = gan.gen_losses\n","    dis_losses = gan.dis_losses\n","    d_accuracies = gan.d_accuracies\n","    gen_accuracies = gan.gen_accuracies\n","    print(\"**********************************\")\n","    print(gen_losses)\n","    print(dis_losses)\n","    print(d_accuracies)\n","    print(gen_accuracies)\n","    gen_losses, dis_losses, d_accuracies, gen_accuracies = gan.get_training_metrics()\n","    metrics_path = save_path + 'training_metrics.csv'\n","    with open(metrics_path, 'w', newline='') as csvfile:\n","        metric_writer = csv.writer(csvfile, delimiter=',',\n","                                   quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n","        # Write headers\n","        metric_writer.writerow(['Epoch', 'Generator Loss', 'Discriminator Loss', 'Discriminator Accuracy', 'Generator Accuracy'])\n","\n","        # Write metrics for each epoch\n","        for epoch in range(len(gen_losses)):\n","            metric_writer.writerow([epoch+1, gen_losses[epoch], dis_losses[epoch], d_accuracies[epoch], gen_accuracies[epoch]])\n","\n","\n","if __name__ == '__main__':\n","    run()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IOaV53XlCieY"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","# Plotting\n","plt.figure(figsize=(14, 7))\n","\n","# Generator and Discriminator accuracy\n","plt.plot(data['Epoch'], data['Generator Accuracy'], label='Generator Accuracy', marker='o')\n","plt.plot(data['Epoch'], data['Discriminator Accuracy'], label='Discriminator Accuracy', marker='x')\n","\n","# Labeling\n","plt.title('Generator and Discriminator Accuracy over Epochs')\n","plt.xlabel('Epoch')\n","plt.ylabel('Accuracy')\n","plt.legend()\n","plt.grid(True)\n","\n","plt.show()\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}